# Set of models with accuracy issues, because of PA
EleutherAI/pythia-160m
bigscience/bloomz-1b7
bigscience/bloomz-560m
databricks/dolly-v2-3b
tiiuae/falcon-rw-7b
bigcode/starcoder2-3b
openbmb/MiniCPM-2B-sft-bf16
openbmb/MiniCPM-2B-dpo-bf16
Qwen/Qwen-7B
Qwen/Qwen-7B-Chat
Qwen/Qwen1.5-0.5B
Qwen/Qwen1.5-7B-Chat
internlm/internlm-chat-7b
BAAI/Aquila-7B
internlm/internlm2-7b
openchat/openchat_3.5
lmsys/vicuna-7b-v1.5
lmsys/longchat-7b-v1.5-32k
BAAI/AquilaChat2-7B
BAAI/AquilaChat-7B
baichuan-inc/Baichuan-7B
tiiuae/falcon-7b
microsoft/Phi-3-mini-128k-instruct
microsoft/Phi-3-mini-4k-instruct#
nomic-ai/gpt4all-mpt
mosaicml/mpt-7b
# Set of models, failed because of C++ Cont. Batching
# RuntimeError: Check 'rt_info.find("eos_token_id") != rt_info.end(): facebook/incoder-1B
#
# Set of models, which require support in optimum-intel / transformers / models repositories:
# https://huggingface.co/Salesforce/xgen-7b-8k-base/discussions/32: Salesforce/xgen-7b-8k-base
# Trying to export a jais model, that is a custom or unsupported architecture: core42/jais-13b-chat
# IndexError: tuple index out of range: facebook/blenderbot-3B
# `pip install flash_attn`: OrionStarAI/Orion-14B-Base
# Xeon only: reports IP_ADDRESS on Optimum inference: allenai/OLMo-1B-hf
# Xeon only: reports IP_ADDRESS on Optimum inference: allenai/OLMo-7B-hf
#
# Set of models, failed because of CPU limitation
# head size must be multiple of 16, current: 100: pankajmathur/orca_mini_3b
# head size must be multiple of 16, current: 100: openlm-research/open_llama_3b
# head size must be multiple of 16, current: 100: openlm-research/open_llama_3b_v2
#
# Set of failed models, because of PA:
# 'start' input is not a scalar: google/pegasus-big_patent
# 'start' input is not a scalar: google/pegasus-large
# 'stop' input is not a scalar: Salesforce/codegen2-1b
# Model references undeclared parameters: opset1::Parameter attention_mask () -> (i64[?,?]): bigcode/starcoderbase-3b
# Model references undeclared parameters: opset1::Parameter attention_mask () -> (i64[?,?]): bigcode/gpt_bigcode-santacoder
# Model references undeclared parameters: opset1::Parameter attention_mask () -> (i64[?,?]): facebook/opt-350m
#
# Set of models, failed because of OpenVINO Tokenizers:
# https://jira.devtools.intel.com/browse/CVS-142063: rinna/bilingual-gpt-neox-4b
#
# Set of 13B, 30B abd 70B models:
EleutherAI/gpt-neox-20b
# big model, not tried: core42/jais-13b
# see optimum: core42/jais-13b-chat
# big model, not tried: young-geng/koala
mistralai/Mixtral-8x7B-v0.1
# big model, not tried: mistralai/Mixtral-8x7B-Instruct-v0.1
# big model, not tried: mosaicml/mpt-30b
# see optimum: OrionStarAI/Orion-14B-Base
# big model, not tried: OrionStarAI/Orion-14B-Chat
# big model, not tried: Qwen/Qwen1.5-MoE-A2.7B
Qwen/Qwen1.5-MoE-A2.7B-Chat
xverse/XVERSE-MoE-A4.2B
#
# Set of passed models:
microsoft/phi-2
microsoft/phi-1_5
EleutherAI/gpt-neo-125m
EleutherAI/gpt-neo-125m
EleutherAI/gpt-neo-1.3B
EleutherAI/gpt-j-6b
baichuan-inc/Baichuan2-7B-Chat
THUDM/chatglm2-6b
THUDM/chatglm3-6b
google/gemma-2b
google/gemma-7b
openai-community/gpt2
openai-community/gpt2-xl
gpt2
gpt2-xl
nomic-ai/gpt4all-j
stabilityai/stablelm-3b-4e1t
stabilityai/stablelm-2-zephyr-1_6b
meta-llama/Llama-2-7b-hf
meta-llama/Meta-Llama-3-8B-Instruct
meta-llama/CodeLlama-7b-hf
lmsys/vicuna-7b-v1.3
mistralai/Mistral-7B-v0.1
mistralai/Mistral-7B-Instruct-v0.1
01-ai/Yi-6B
Salesforce/codegen-350M-multi
Salesforce/codegen-350M-nl
togethercomputer/RedPajama-INCITE-Chat-3B-v1
# passed, but with export=False: OpenVINO/codegen25-7b-multi-fp16-ov
#
# Set of invalid models, because of HF:
# HF: Exception: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 78 column 3: xverse/XVERSE-7B-Chat